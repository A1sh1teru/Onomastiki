{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\velic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesTeachers = pd.read_csv('D:/Programming/Onomastiki/flaskProject/templates/data/namesTeachers.csv')\n",
    "namesStudents = pd.read_csv('D:/Programming/Onomastiki/flaskProject/templates/data/namesStudents.csv')\n",
    "namesCombined = pd.read_csv('D:/Programming/Onomastiki/flaskProject/templates/data/namesCombined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    # Применение токенизации к столбцу \"Meaning\"\n",
    "    tokenized_meanings = []\n",
    "    for text in data[\"Meaning\"]:\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc if not token.is_punct]  # Получение токенов без пунктуации\n",
    "        tokenized_meanings.append(tokens)\n",
    "\n",
    "    # Добавление токенизированных данных в DataFrame\n",
    "    data[\"Tokenized_Meaning\"] = tokenized_meanings\n",
    "    return data\n",
    "\n",
    "# Пример использования функции\n",
    "colName = 'Meaning'\n",
    "namesTeachers = tokenize_data(namesTeachers)\n",
    "namesStudents = tokenize_data(namesStudents)\n",
    "namesCombined = tokenize_data(namesCombined)\n",
    "# namesCombined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "# Обучение моделей для каждого источника\n",
    "model_students = Word2Vec(sentences=namesStudents['Tokenized_Meaning'].tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
    "model_teachers = Word2Vec(sentences=namesTeachers['Tokenized_Meaning'].tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
    "model_combined = Word2Vec(sentences=namesCombined['Tokenized_Meaning'].tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model_students.save(\"word2vec_students.model\")\n",
    "model_teachers.save(\"word2vec_teachers.model\")\n",
    "model_combined.save(\"word2vec_combined.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_meanings(data, model_path):\n",
    "    vectorized_meanings = []\n",
    "    # Загрузка ранее сохраненной модели Word2Vec\n",
    "    model = Word2Vec.load(model_path)\n",
    "    # Векторизация столбца \"Tokenized_Meaning\"\n",
    "    for tokens in data[\"Tokenized_Meaning\"]:\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        mean_vector = np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "        vectorized_meanings.append(mean_vector)\n",
    "    # Добавление векторизованных значений обратно в DataFrame\n",
    "    data[\"Vectors\"] = vectorized_meanings\n",
    "    return data\n",
    "\n",
    "# Векторизация данных с использованием функции vectorize_meanings\n",
    "model_students_path = \"word2vec_students.model\"\n",
    "model_teachers_path = \"word2vec_teachers.model\"  \n",
    "model_combined_path = \"word2vec_combined.model\"# Путь к обученной модели Word2Vec\n",
    "namesStudents = vectorize_meanings(namesStudents, model_students_path)\n",
    "namesTeachers = vectorize_meanings(namesTeachers, model_teachers_path)\n",
    "namesCombined = vectorize_meanings(namesCombined, model_combined_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def k_means(data, k):\n",
    "    # Инициализация центроид\n",
    "    centroids = data[np.random.choice(range(len(data)), size=k)]\n",
    "\n",
    "    while True:\n",
    "        # Выделение кластеров\n",
    "        clusters = [[] for i in range(k)]\n",
    "        for point in data:\n",
    "            distances = [np.linalg.norm(point - centroid) for centroid in centroids]\n",
    "            closest_centroid_idx = np.argmin(distances)\n",
    "            clusters[closest_centroid_idx].append(point)\n",
    "\n",
    "        # Обновление центроид\n",
    "        new_centroids = []\n",
    "        for cluster in clusters:\n",
    "            if len(cluster) > 0:\n",
    "                cluster_mean = np.mean(cluster, axis=0)\n",
    "                new_centroids.append(cluster_mean)\n",
    "\n",
    "        # Проверка на сходимость\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids\n",
    "\n",
    "# Пример использования\n",
    "students = np.array(namesStudents['Vectors'].tolist(), dtype=float)\n",
    "teachers = np.array(namesTeachers['Vectors'].tolist(), dtype=float)\n",
    "combination = np.array(namesCombined['Vectors'].tolist(), dtype=float)\n",
    "k = 1\n",
    "centroids_students = k_means(students, k)\n",
    "centroids_teachers = k_means(teachers, k)\n",
    "centroids_comb = k_means(combination, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_vector(centroid, vectors):\n",
    "    min_distance = float('inf')\n",
    "    closest_vector = None\n",
    "    \n",
    "    for vector in vectors:\n",
    "        distance = np.linalg.norm(centroid - vector)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_vector = vector\n",
    "    \n",
    "    return closest_vector\n",
    "\n",
    "vectorS = namesStudents['Vectors']\n",
    "vectorT = namesTeachers['Vectors']\n",
    "vectorC = namesCombined['Vectors']\n",
    "closest_vector_students = find_closest_vector(centroids_students, vectorS)\n",
    "closest_vector_teachers = find_closest_vector(centroids_teachers, vectorT)\n",
    "closest_vector_combined = find_closest_vector(centroids_comb, vectorC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ближайший токен: с\n"
     ]
    }
   ],
   "source": [
    "decodedStudent = closest_vector_students\n",
    "similar_word = model_students.wv.most_similar(positive=[closest_vector_students], topn=1)\n",
    "print('Ближайший токен:', similar_word[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ближайший токен: поток\n"
     ]
    }
   ],
   "source": [
    "decodedTeacher = closest_vector_teachers\n",
    "similarW2 = model_teachers.wv.most_similar(positive=[closest_vector_teachers], topn=1)\n",
    "print('Ближайший токен:', similarW2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ближайший токен: Бога\n"
     ]
    }
   ],
   "source": [
    "decodedCombined = closest_vector_combined\n",
    "similarW1 = model_combined.wv.most_similar(positive=[decodedCombined], topn=1)\n",
    "print('Ближайший токен:',similarW1[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
